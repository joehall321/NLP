{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Coursework Assignment\n",
    "\n",
    "CW heavily weighted on preprocessing steps (Feature extraction)!!!\n",
    "\n",
    "Scientific method: \n",
    "Motivate, justify, explain \n",
    "\n",
    "Structure\n",
    "Step 1 Baseline model: \n",
    "    Baseline model – select basic ML model (Naïve base) \n",
    "        Research papers, justify why we are starting with NB – proven results for text classification (spam filtering).\n",
    "Step 2 Feature engineering:\n",
    "    Analyse and compare different feature extraction methods and preprocessing. Input different features into same model. Analyse results. Compare feature extraction and preprocessing using same model. See what produces best results.  \n",
    "        Preprocessing – tokenization and linguistic analysis (Zipf's law). \n",
    "        Latent Dirichlet Allocation (LDA) - Unsupervised.   \n",
    "        Feature extraction (Information retrieval techniques) – feature vector construction: \n",
    "        Word frequency. \n",
    "        Word classification frequency (Num of neg/pos words). \n",
    "        Stemming and Lemmatization  \n",
    "        PoS  \n",
    "        TDFG, bigram models.  \n",
    "Step 3:\n",
    "    Then choose preprocessing based on results from baseline model. Expand baseline model to more complex model. \n",
    "    Implement different complex models to explore results and what works best with same preprocessing. Compare methods from different areas, e.g SVM, clustering model and NN.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1>Introduction:</h1>\n",
    "Creating software which can distinguish between positive and negative movie reviews involves the task of binary classification using machine learning. Arguably, the most important aspect of classifying a negative or positive review, is the review's features. These features are used to define the review's characteristics and general opinion, helping one choose between negative or positive. In other words, the features determine how a review is interpreted. Therefore, the analysis and construction of these features is a key aspect to the success of building a classification model. Furthermore, classification falls into a branch of machine learning called supervised learning. Supervised learning is an area involving the construction of a statistical model from labeled data sets, which then can be used to predict the class of a new unseen data point. This model is constructed from features which are engineered from the review text. There are many different techniques and methods within the natural language processing (NLP) paradigm which can be used to extract useful features from language. The success and accuracy of the classification model is heavily reliant on the extraction of useful and revealing features. This is because the model training will construct a prediction function using the statistical properties of these features. \n",
    "<br> <br>\n",
    "\n",
    "<img src=\"images/text_classificaton_model.png\" width=\"50%\" alt=\"Source: https://monkeylearn.com/text-classification/\"> \n",
    "\n",
    "\n",
    "The above image shows the process architecture of training a ML model for text binary classification.\n",
    "\n",
    "More specifically, the binary classification task could be approached in two similar ways. The first, training a model from Boolean labeled data, so it can predict either positive or negative. The second, training a model from review rating labelled data, so it can multi-class prediction between 0-4 and 7-9. Thereafter, a threshold can be set to produce a binary classification of positive or negative. Furthermore, both approaches will require sufficient and equal data points for each class so the model training remains equally distributed.\n",
    "\n",
    "Feature engineering is a key process step in constructing a successful text classifier. The text domain contains a high dimensionality of feature space, each word and each phrase can be interpreted and translated into many meaningful representations. Some of which may not be relevant and beneficial for the review classification task. Irrelevant features may add noise and reduce model prediction accuracy. The feature extraction process is an opportunity to reduce the high dimensionality feature space of the review text, encouraging improvement in the efficiency and accuracy of our classifier. Therefore, we will need to test different feature vectors by evaluating their relevance and usefulness using some process and metrics. According to John, Kohavi, and Pfleger (1994) [2], there are two main types of feature selection methods: wrappers and filters. The wrapper approach tests different features with the same baseline model, evaluating performance via model accuracy, allowing the identification and construction of an optimal feature vector which can be used to train a future model. Whereas filters make use of evaluation metrics to determine a features ability to differentiate between classes. The later method is described to be much more suitable when identifying features for text classification, due to wrappers requiring the training of a classifier for each feature subset, becoming far more computationally expensive than filters [1]. However as Mladenic showed when using the filter selection method, the choice of evaluation metrics is paramount. A suitable metric for this text classification problem should consider the problem domain and algorithm as it has been proven in literature that .  \n",
    "\n",
    "As a student of NLP, I will be using a hybrid approach of both wrappers and filters to help investigate the features but also compare the two feature selection processes. In contradiction to Jingnian Chen, Houkuan Huang, Shengfeng Tian, Youli Qu [1], when taking the wrapper approach, I estimate that the computational expense of training a classifier for each subset of feature vectors will be manageable. Additionally, I am also intrigued how the feature testing results of using a baseline model, compared to the results of using an evaluative metric. Does the evaluation metric reflect the accuracy produced by the model? Does the metric imply the relative performance of the baseline model? Furthermore, I will then also be able to investigate different evaluation metrics.  \n",
    " \n",
    "The goal of my hybrid approach and investigative comparison is to verify the integrity of both wrappers and filters as feature selection methods for text classification models, and most importantly engineer features which encourage the best performance of our text classifier. \n",
    "\n",
    "<br><hr>\n",
    "<h1>Feature Analysis & Selection:</h1>\n",
    "<h5>Wrapper - Baseline model:</h5>As my wrapper and baseline model, I will be using a Naive Bayes classifier. This is because the Naive Bayes classifier has been demonstrated to achieve relatively impressive results in text classification problems such as spam detection. The Naive Bayes classifier is also computationally inexpensive so overhead for training against many feature vector subsets will be relatively efficient. I will be using my baseline model to measure the effects of feature vectors on predictive success, and utimatley identify a feature vector which delivers optimal results. \n",
    "\n",
    "<h5>Filters - Evaluation metrics:</h5><br>\n",
    "As Mladenic showed [3], a feature selection metric may only be effective for a specific domain and less effective than alternative metrics in another. Therefore it is important to carefully choose an evaluation metric considering our domain of binary classification. Mladenic and Grobelnik [3] demonstrated that binary-class Odds Ratio performed effective measurments when testing feature vectors for the Naive Bayes Classifier. The Odds Ratio metric can also be applied to multi-class datasets, allowing the investigation of multi-class prediction occumpanied by a binary theshold. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words:  48388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1176/4000 [03:57<09:30,  4.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMNB \u001b[39m\u001b[39m%\u001b[39m\u001b[39m):\u001b[39m\u001b[39m\"\u001b[39m, metrics\u001b[39m.\u001b[39maccuracy_score(y_test, y_pred)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m    100\u001b[0m preprocess \u001b[39m=\u001b[39m PreProcess()\n\u001b[0;32m--> 101\u001b[0m ratings, feature_vector \u001b[39m=\u001b[39m preprocess\u001b[39m.\u001b[39;49mextractFeatures([\u001b[39m'\u001b[39;49m\u001b[39mword frequency\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    102\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(feature_vector, ratings, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m    103\u001b[0m y_pred \u001b[39m=\u001b[39m MNB_Classifier(X_train, X_test, y_train, y_test)\n",
      "Cell \u001b[0;32mIn [2], line 80\u001b[0m, in \u001b[0;36mPreProcess.extractFeatures\u001b[0;34m(self, feature_list)\u001b[0m\n\u001b[1;32m     78\u001b[0m classes \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(review_data\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mword frequency\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m feature_list:\n\u001b[0;32m---> 80\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollectWordFreq(review_data)\n\u001b[1;32m     81\u001b[0m     feature_vector\u001b[39m=\u001b[39m[]\n\u001b[1;32m     82\u001b[0m     ratings_vector\u001b[39m=\u001b[39m[]\n",
      "Cell \u001b[0;32mIn [2], line 54\u001b[0m, in \u001b[0;36mPreProcess.collectWordFreq\u001b[0;34m(self, review_data)\u001b[0m\n\u001b[1;32m     52\u001b[0m review \u001b[39m=\u001b[39m review_data\u001b[39m.\u001b[39mget(filename)\n\u001b[1;32m     53\u001b[0m \u001b[39m# Append word frequency to feature dict.\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m review_word_freq \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcountWordFreq(review[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     55\u001b[0m review[\u001b[39m2\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mword frequency\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39mreview_word_freq\n",
      "Cell \u001b[0;32mIn [2], line 67\u001b[0m, in \u001b[0;36mPreProcess.countWordFreq\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word \u001b[39min\u001b[39;00m word_freq_dict:\n\u001b[1;32m     65\u001b[0m         \u001b[39m# Lapace smoothing\u001b[39;00m\n\u001b[1;32m     66\u001b[0m         word_freq_dict[word]\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m collections\u001b[39m.\u001b[39;49mOrderedDict(\u001b[39msorted\u001b[39;49m(word_freq_dict\u001b[39m.\u001b[39;49mitems()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re \n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "class PreProcess():\n",
    "\n",
    "    def __init__(self):\n",
    "        # Read data and format into data structure\n",
    "        self.neg_reviews = ['data/neg/'+x for x in os.listdir('data/neg')]\n",
    "        self.pos_reviews = ['data/pos/'+x for x in os.listdir('data/pos')]\n",
    "        # Data structure dic(file_name, ([text], {features}, rating))\n",
    "        self.vocabulary = {}\n",
    "        self.data = self.readReviewData(self.neg_reviews) | self.readReviewData(self.pos_reviews)\n",
    "        print(\"Total unique words: \",len(self.vocabulary.keys()))\n",
    "        \n",
    "    # Reads files, removes html and returns dict(key=filename,list of words) \n",
    "    def readReviewData(self, files):\n",
    "        review_dict = {}\n",
    "        for txt_file in files:\n",
    "            with open(txt_file) as txt:\n",
    "                lines = txt.readlines()\n",
    "            # Strip HTML and tokenise.\n",
    "            text = re.sub('<[^<]+?>', ' ',lines[0].replace(\",\",\", \").replace(\".\",\". \"))\n",
    "            text = nltk.tokenize.word_tokenize(text, language='english')\n",
    "            # Collect all words into vocabulary\n",
    "            self.updateVocabulary(text)\n",
    "            rating = self.getRating(txt_file)\n",
    "            # Second index is for rating.\n",
    "            # Third index is for features.\n",
    "            review_dict[txt_file] = (text,rating,{})\n",
    "        return review_dict\n",
    "\n",
    "    def updateVocabulary(self, text):\n",
    "        for word in text:\n",
    "            count = self.vocabulary.get(word,0)\n",
    "            self.vocabulary[word] = count + 1 \n",
    "\n",
    "    def getRating(self, file_name):\n",
    "        return int(file_name[len(file_name)-5])\n",
    "\n",
    "    def collectWordFreq(self, review_data):\n",
    "        # Loop through texts and count frequency for each rating class.\n",
    "        for filename in tqdm(review_data):\n",
    "            review = review_data.get(filename)\n",
    "            # Append word frequency to feature dict.\n",
    "            review_word_freq = self.countWordFreq(review[0])\n",
    "            review[2]['word frequency']=review_word_freq\n",
    "\n",
    "    # Count frequency of words in text. \n",
    "    def countWordFreq(self, text):\n",
    "        word_freq_dict = {}\n",
    "        for word in text:\n",
    "            count = word_freq_dict.get(word,1)\n",
    "            word_freq_dict[word] = count+1\n",
    "        for word in self.vocabulary:\n",
    "            if not word in word_freq_dict:\n",
    "                # Lapace smoothing\n",
    "                word_freq_dict[word]=1\n",
    "        return collections.OrderedDict(sorted(word_freq_dict.items()))\n",
    "\n",
    "    def zipf_analysis(self, review_data):\n",
    "        # Process 3 different datasets to train model:\n",
    "        # 1 - Head, body & tail.\n",
    "        # 2 - Head & body.\n",
    "        # 3 - Body & tail.\n",
    "        pass\n",
    "\n",
    "    def extractFeatures(self, feature_list):\n",
    "        review_data = self.data\n",
    "        classes = sorted(review_data.keys())\n",
    "        if 'word frequency' in feature_list:\n",
    "            self.collectWordFreq(review_data)\n",
    "            feature_vector=[]\n",
    "            ratings_vector=[]\n",
    "            for review in review_data:\n",
    "                feature_vector.append(list(review_data[review][2]['word frequency'].values()))\n",
    "                ratings_vector.append(review_data[review][1])\n",
    "            return ratings_vector, feature_vector\n",
    "    \n",
    "def MNB_Classifier(X_train, X_test, y_train, y_test):\n",
    "    mnb_clf = MultinomialNB(force_alpha=True, verbose=True)\n",
    "    print(\"Training Multinomial Naive Bayes Classifier.\")\n",
    "    mnb_clf.fit(X_train,y_train)\n",
    "    y_pred = mnb_clf.predict(X_test)\n",
    "    analyseResults(y_test, y_pred)\n",
    "\n",
    "def analyseResults(y_test, y_pred):\n",
    "    print(\"\\nConfusion Matrix: \")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"MNB %):\", metrics.accuracy_score(y_test, y_pred)*100)\n",
    "            \n",
    "preprocess = PreProcess()\n",
    "ratings, feature_vector = preprocess.extractFeatures(['word frequency'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vector, ratings, test_size=0.2, random_state=42)\n",
    "y_pred = MNB_Classifier(X_train, X_test, y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "[1] Jingnian Chen, Houkuan Huang, Shengfeng Tian, Youli Qu,\n",
    "Feature selection for text classification with Naïve Bayes,\n",
    "Expert Systems with Applications,\n",
    "Volume 36, Issue 3, Part 1,2009,pp. 5432-5435,\n",
    "\n",
    "[2] G.H. John, R. Kohavi, K. Pfleger\n",
    "Irrelevant Features and the Subset Selection Problem\n",
    "Proceedings of the 11th International Conference on machine learning, Morgan Kaufmann, San Francisco (1994), pp. 121-129\n",
    "\n",
    "[3] Mladenic, D., & Grobelnik, M., 1999. \n",
    "Feature selection for unbalanced class distribution and Naive Bayes. \n",
    "In Proceedings of 16th international conference on machine learning (pp. 258–267). San Francisco."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
